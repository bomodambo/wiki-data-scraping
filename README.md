# Wikipedia Data Scraping: Efficient Extraction and Database Integration

## Project Overview
The Wikipedia data harvest project focuses on the extraction of data from Wikipedia, specifically targeting information about universities. The primary objective is to automate the retrieval and storage of university-related data into various databases, initially including MySQL, Postgres, and SQL Server.

## Project Learning Opportunities
- Implement web scraping using Python libraries such as Beautiful Soup
- Set up and configure different databases using Docker
- Understand HTML structure to navigate and extract relevant information from Wikipedia pages
- Efficiently load scraped data into various databases
- Explore techniques for managing database connections and transactions

## Learning Skills
- Web Scraping
- Python Programming
- Database Management
- SQL
- Knowledge of Diverse Database Systems

## Executive Summary
The Wikipedia data harvest project automates the extraction of university-related data from Wikipedia and its integration into various databases like MySQL, Postgres, and SQL Server. This automation reduces the manual effort required to collect and update information, ensuring scalability and accuracy in educational data management.

## Problem Statement
The absence of an automated, scalable solution for acquiring comprehensive university information presents a significant challenge in educational data management. Manual data collection from Wikipedia is time-consuming, error-prone, and lacks scalability. This project addresses the need for a streamlined process to systematically harvest university data from Wikipedia and integrate it into multiple database platforms.

## Objectives
- **Implement Web Scraping:** Develop a robust web scraping module using Python libraries to extract relevant university data from Wikipedia pages.
- **Database Setup and Configuration:** Set up and configure databases using Docker to establish a foundation for data storage.
- **Database Schema Design:** Design and implement appropriate database schemas to accommodate the scraped university data.
- **Database Integration:** Develop scripts to seamlessly load scraped data into the designated databases.
- **SQL Query Development:** Craft SQL queries for creating, reading, updating, and deleting data in the databases.
- **Cost Efficiency:** Optimize resource utilization and costs by leveraging Distributed SQL Queries.

## Data Pipeline
(Insert Data Pipeline Diagram Here)

## Tech Stack
- **SQL Server**
- **PostgreSQL**
- **MySQL**
- **Python**
- **Docker**

## Development
- **Python:** Used for web scraping and database integration.
- **Docker:** Utilized to set up and configure different database systems.

## Project Scope
- Understanding the project overview and architecture.
- Systematic extraction of relevant university information from Wikipedia pages.
- Utilizing Docker containers for different database systems.
- Loading data into different databases from the Python environment.

![image](https://github.com/user-attachments/assets/655b450b-492d-4190-9213-15a18dd5cdcf)

